{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Procesamiento** **del** **dataset** **input** **para** **la** **segunda** **fase** **de** **entrenamiento** **NER**\n",
        "\n",
        "En el siguiente m√≥dulo se preprocesa el dataset utilizado para la primera iteraci√≥n de entrenamiento del modelo NER, utilizando las conclusiones obtenidas deeste para mejorar el dataset de entrenamiento y conseguir de este modo un modelo m√°s robusto. La primera etapa, consistente en cargar el dataset y obtener mediante un pipeline un dataset limpio para usar en el entrenemiento, es id√©ntico al del m√≥dulo de preprocesamiento (m√≥dulo 1)"
      ],
      "metadata": {
        "id": "KrzjN3LBvze2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62SPIbhTVqw2",
        "outputId": "784558aa-b078-459e-9de1-49b178df79aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install -q openai ipywidgets python-dotenv\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "nKIwyV2Pmjj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "# Leerlo de vuelta\n",
        "contenido = Path(\"recetas.json\").read_text(encoding=\"utf-8\")\n",
        "recetas_cargadas = json.loads(contenido)"
      ],
      "metadata": {
        "id": "DKP85ARa1kD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion para a√±adir resumen a la receta\n",
        "import re\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "def agregar_resumen_a_receta(receta: Dict[str, Any], clave_pasos: str = \"pasos\", clave_resumen: str = \"resumen\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    A√±ade receta[clave_resumen] uniendo receta[clave_pasos] (lista de strings)\n",
        "    en una sola cadena, separadas por punto. Limpia puntuaci√≥n final duplicada.\n",
        "\n",
        "    - Si no hay pasos o no es una lista de strings, pone resumen = \"\".\n",
        "    - Devuelve el propio diccionario (mutado) por conveniencia.\n",
        "    \"\"\"\n",
        "    pasos = receta.get(clave_pasos) or []\n",
        "    if not isinstance(pasos, list):\n",
        "        receta[clave_resumen] = \"\"\n",
        "        return receta\n",
        "\n",
        "    limpios: List[str] = []\n",
        "    for p in pasos:\n",
        "        if not isinstance(p, str):\n",
        "            continue\n",
        "        s = p.strip()\n",
        "        # quita puntuaci√≥n final para evitar \"..\"\n",
        "        s = re.sub(r\"\\s*([.;:!?‚Ä¶]+)\\s*$\", \"\", s)\n",
        "        if s:\n",
        "            limpios.append(s)\n",
        "\n",
        "    # une con \". \" y a√±ade punto final si hay contenido\n",
        "    resumen = \". \".join(limpios)\n",
        "    if resumen:\n",
        "        resumen += \".\"\n",
        "    receta[clave_resumen] = resumen\n",
        "    return receta"
      ],
      "metadata": {
        "id": "UGa16e5s1Vyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for receta in recetas_cargadas:\n",
        "  receta = agregar_resumen_a_receta(receta)"
      ],
      "metadata": {
        "id": "qMs5MJUlAfsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizaci√≥n del dataset\n",
        "\n",
        "import re, unicodedata\n",
        "from typing import Tuple\n",
        "\n",
        "_STOPWORDS = {\"de\",\"del\",\"la\",\"el\",\"las\",\"los\",\"al\",\"y\",\"en\",\"con\",\"para\"}\n",
        "\n",
        "def _quitar_tildes(txt: str) -> str:\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", txt) if unicodedata.category(c) != \"Mn\")\n",
        "\n",
        "def _build_norm_index_map(original: str) -> Tuple[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Devuelve (texto_normalizado, idx_map) donde idx_map[i] es el √≠ndice del\n",
        "    car√°cter original correspondiente al car√°cter normalizado i.\n",
        "    \"\"\"\n",
        "    norm_chars, idx_map = [], []\n",
        "    for i, ch in enumerate(original):\n",
        "        base = _quitar_tildes(ch).lower()\n",
        "        for _ in base:\n",
        "            norm_chars.append(_)\n",
        "            idx_map.append(i)\n",
        "    return \"\".join(norm_chars), idx_map\n",
        "\n",
        "def _plural_es_basico(pal: str) -> str:\n",
        "    w = _quitar_tildes(pal.lower())\n",
        "    if not w:\n",
        "        return w\n",
        "    if w.endswith(\"z\"):\n",
        "        return w[:-1] + \"ces\"\n",
        "    if w.endswith((\"s\",\"x\")):\n",
        "        return w  # heur√≠stica: la mayor√≠a quedan invariables\n",
        "    if re.search(r\"[aeiou]$\", w):\n",
        "        return w + \"s\"\n",
        "    return w + \"es\"\n",
        "\n",
        "def _plural_de_frase(ing: str) -> str:\n",
        "    toks = ing.lower().split()\n",
        "    if not toks:\n",
        "        return ing.lower()\n",
        "    idx = len(toks) - 1\n",
        "    while idx >= 0 and toks[idx] in _STOPWORDS:\n",
        "        idx -= 1\n",
        "    if idx < 0:\n",
        "        return ing.lower()\n",
        "    toks[idx] = _plural_es_basico(toks[idx])\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def asociar_ingredientes_en_resumen(\n",
        "    receta: Dict[str, Any],\n",
        "    usar_limites_palabra: bool = True,\n",
        "    detectar_plural: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Muy simple: si un ingrediente (en singular) est√° en receta['resumen'],\n",
        "    lo asocia. Tambi√©n captura el plural (heur√≠stico) y es insensible a tildes.\n",
        "    A√±ade:\n",
        "      - receta['ingredientes_en_resumen'] (lista deduplicada)\n",
        "      - receta['spans_resumen'] con {ingredient, start, end, surface}\n",
        "    Offsets 0-based sobre el resumen original. 'end' exclusivo.\n",
        "    \"\"\"\n",
        "    resumen = (receta.get(\"resumen\") or \"\")\n",
        "    ingredientes = receta.get(\"ingredientes\") or []\n",
        "    if not isinstance(resumen, str) or not isinstance(ingredientes, list):\n",
        "        receta[\"ingredientes_en_resumen\"] = []\n",
        "        receta[\"spans_resumen\"] = []\n",
        "        return receta\n",
        "\n",
        "    # Texto normalizado + mapa a original (insensible a tildes)\n",
        "    resumen_norm, idx_map = _build_norm_index_map(resumen)\n",
        "\n",
        "    presentes: List[str] = []\n",
        "    spans: List[Dict[str, Any]] = []\n",
        "\n",
        "    for ing in ingredientes:\n",
        "        ing_sing = (ing or \"\").strip()\n",
        "        if not ing_sing:\n",
        "            continue\n",
        "\n",
        "        # variantes: singular + plural (heur√≠stico)\n",
        "        variantes = [ing_sing]\n",
        "        if detectar_plural:\n",
        "            pl = _plural_de_frase(ing_sing)\n",
        "            if pl and pl != ing_sing:\n",
        "                variantes.append(pl)\n",
        "\n",
        "        # buscamos cada variante sobre el texto normalizado\n",
        "        seen_positions = set()\n",
        "        for var in variantes:\n",
        "            var_norm = _quitar_tildes(var).lower()\n",
        "            if not var_norm:\n",
        "                continue\n",
        "\n",
        "            if usar_limites_palabra:\n",
        "                pat = re.compile(rf\"(?<!\\w){re.escape(var_norm)}(?!\\w)\")\n",
        "                matches = list(pat.finditer(resumen_norm))\n",
        "                if matches and ing_sing not in presentes:\n",
        "                    presentes.append(ing_sing)\n",
        "                for m in matches:\n",
        "                    s_n, e_n = m.span()\n",
        "                    s = idx_map[s_n]\n",
        "                    e = idx_map[e_n - 1] + 1\n",
        "                    if (s, e) in seen_positions:\n",
        "                        continue\n",
        "                    seen_positions.add((s, e))\n",
        "                    spans.append({\n",
        "                        \"ingredient\": ing_sing,\n",
        "                        \"start\": s,\n",
        "                        \"end\": e,\n",
        "                        \"surface\": resumen[s:e]\n",
        "                    })\n",
        "            else:\n",
        "                start_n = 0\n",
        "                found = False\n",
        "                while True:\n",
        "                    idx_n = resumen_norm.find(var_norm, start_n)\n",
        "                    if idx_n == -1:\n",
        "                        break\n",
        "                    s = idx_map[idx_n]\n",
        "                    e = idx_map[idx_n + len(var_norm) - 1] + 1\n",
        "                    if (s, e) not in seen_positions:\n",
        "                        seen_positions.add((s, e))\n",
        "                        spans.append({\n",
        "                            \"ingredient\": ing_sing,\n",
        "                            \"start\": s,\n",
        "                            \"end\": e,\n",
        "                            \"surface\": resumen[s:e]\n",
        "                        })\n",
        "                        found = True\n",
        "                    start_n = idx_n + len(var_norm)\n",
        "                if found and ing_sing not in presentes:\n",
        "                    presentes.append(ing_sing)\n",
        "\n",
        "    # deduplicar presentes manteniendo orden\n",
        "    seen_ing = set()\n",
        "    presentes = [x for x in presentes if not (x in seen_ing or seen_ing.add(x))]\n",
        "\n",
        "    receta[\"ingredientes_en_resumen\"] = presentes\n",
        "    receta[\"spans_resumen\"] = sorted(spans, key=lambda s: (s[\"start\"], s[\"end\"]))\n",
        "    return receta\n"
      ],
      "metadata": {
        "id": "tI2dhbcF3btS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for receta in recetas_cargadas:\n",
        "  asociar_ingredientes_en_resumen(receta,usar_limites_palabra=True)"
      ],
      "metadata": {
        "id": "W_zm_tfKAvfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenci√≥n de la lista de los ingredientes usados en el primer entrenamiento NER (con m√°s de 50 aparaciones)\n",
        "def top_ingredientes_frecuentes(\n",
        "    recetas: List[Dict[str, Any]],\n",
        "    umbral: int = 50,\n",
        "    clave_spans: str = \"spans_resumen\",\n",
        "    clave_ing: str = \"ingredient\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Cuenta menciones por ingrediente a partir de 'spans_resumen' y\n",
        "    devuelve una lista de ingredientes (strings) que superan el umbral.\n",
        "    - 'umbral' aplica como '>' (estrictamente mayor que).\n",
        "    \"\"\"\n",
        "    c = Counter()\n",
        "    for r in recetas:\n",
        "        for s in (r.get(clave_spans) or []):\n",
        "            ing = s.get(clave_ing)\n",
        "            if ing:\n",
        "                c[ing] += 1\n",
        "\n",
        "    # filtrar > umbral y ordenar desc por conteo, luego alfab√©tico\n",
        "    filtrados = [(ing, n) for ing, n in c.items() if n > umbral]\n",
        "    ordenados = sorted(filtrados, key=lambda x: (-x[1], x[0]))\n",
        "    return [ing for ing, _ in ordenados]"
      ],
      "metadata": {
        "id": "zpM2CR9iClS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingredientes_frecuentes = top_ingredientes_frecuentes(recetas_cargadas)\n",
        "print(ingredientes_frecuentes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o98qqfLgDO4b",
        "outputId": "39240e8e-c0b7-4b46-de34-9b7d4eb275c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sal', 'ajo', 'cebolla', 'patata', 'aceite de oliva', 'huevo', 'tomate', 'pollo', 'harina', 'agua', 'perejil', 'arroz', 'leche', 'mantequilla', 'zanahoria', 'az√∫car', 'piment√≥n', 'aceite', 'puerro', 'pimiento', 'bacalao', 'vino blanco', 'nata', 'pan', 'salm√≥n', 'caldo de pollo', 'cebolleta', 'vinagre', 'caldo', 'pasta', 'queso', 'lim√≥n', 'pimienta', 'caldo de pescado', 'merluza', 'calabac√≠n', 'garbanzo', 'at√∫n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento del dataset para el segundo entrenamiento: se eliminan aquellos ingredientes que el modelo malinterpreta y se simplifican los ingredientes que tienen un significado parecido o id√©ntico solap√°ndolos.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYQsBSjt8kwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminamos de la lista los ingredientes identificados como problem√°ticos en la primera iteraci√≥n de entrenamiento:\n",
        "\n",
        "- **cebolleta**: al tokenizar el dataset se obtiene el lexema ceboll-, que coincide con el de cebolla, lo cual hace que el modelo erre al tratar de identificarlo.\n",
        "\n",
        "- **pimienta**: al tokenizar el dataset se obtiene el lexema pimient-, que coincide con el de pimiento, lo cual hace que el modelo erre al tratar de identificarlo."
      ],
      "metadata": {
        "id": "LdSeEF7bNQ9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingredientes_frecuentes.remove('cebolleta')\n",
        "ingredientes_frecuentes.remove('pimienta')\n",
        "print(ingredientes_frecuentes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzY0RsoT8iih",
        "outputId": "ca28bce3-3e32-45d8-abdb-4b92bbb7a11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sal', 'ajo', 'cebolla', 'patata', 'aceite de oliva', 'huevo', 'tomate', 'pollo', 'harina', 'agua', 'perejil', 'arroz', 'leche', 'mantequilla', 'zanahoria', 'az√∫car', 'piment√≥n', 'aceite', 'puerro', 'pimiento', 'bacalao', 'vino blanco', 'nata', 'pan', 'salm√≥n', 'caldo de pollo', 'vinagre', 'caldo', 'pasta', 'queso', 'lim√≥n', 'caldo de pescado', 'merluza', 'calabac√≠n', 'garbanzo', 'at√∫n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtramos las recetas de manera que el span solo se haga con los ingredientes de la lista top_frecuentes. Si una receta no contiene ninguno de esos ingredientes se elimina"
      ],
      "metadata": {
        "id": "eSM5s1cHQKpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def filtrar_spans_por_frecuentes(\n",
        "    recetas: List[Dict[str, Any]],\n",
        "    ingredientes_frecuentes: Iterable,  # [\"tomate\", ...] o [(\"tomate\", 123), ...]\n",
        "    clave_spans: str = \"spans_resumen\",\n",
        "    clave_ing: str = \"ingredient\",\n",
        "    actualizar_ingredientes_en_resumen: bool = True,\n",
        "    eliminar_recetas_sin_spans: bool = False,\n",
        "    eliminar_si_ingredientes_en_resumen_vacio: bool = True\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Mantiene en cada receta solo los spans cuyo 'ingredient' est√© en 'ingredientes_frecuentes'.\n",
        "    - Si 'actualizar_ingredientes_en_resumen' es True, recalcula esa lista con los spans filtrados.\n",
        "    - Si 'eliminar_recetas_sin_spans' es True, descarta recetas sin spans tras filtrar.\n",
        "    - Si 'eliminar_si_ingredientes_en_resumen_vacio' es True, descarta recetas cuyo\n",
        "      'ingredientes_en_resumen' quede vac√≠o o nulo tras el filtrado.\n",
        "    Devuelve una **nueva lista** (no modifica la original).\n",
        "    \"\"\"\n",
        "    # Normaliza lista de permitidos a set de strings\n",
        "    allowed = set()\n",
        "    for x in ingredientes_frecuentes or []:\n",
        "        name = x[0] if isinstance(x, (list, tuple)) else x\n",
        "        if isinstance(name, str) and name.strip():\n",
        "            allowed.add(name.strip())\n",
        "\n",
        "    nuevas_recetas: List[Dict[str, Any]] = []\n",
        "    for r in recetas:\n",
        "        spans = r.get(clave_spans) or []\n",
        "        filtrados = [s for s in spans\n",
        "                     if isinstance(s, dict) and s.get(clave_ing) in allowed]\n",
        "\n",
        "        # Clona receta y escribe spans filtrados\n",
        "        r2 = dict(r)\n",
        "        r2[clave_spans] = sorted(\n",
        "            filtrados, key=lambda s: (s.get(\"start\", 0), s.get(\"end\", 0))\n",
        "        )\n",
        "\n",
        "        # Actualiza lista de ingredientes_en_resumen a partir de los spans filtrados\n",
        "        if actualizar_ingredientes_en_resumen:\n",
        "            vistos = set()\n",
        "            ing_list = []\n",
        "            for s in r2[clave_spans]:\n",
        "                lab = s.get(clave_ing)\n",
        "                if lab and lab not in vistos:\n",
        "                    vistos.add(lab)\n",
        "                    ing_list.append(lab)\n",
        "            r2[\"ingredientes_en_resumen\"] = ing_list\n",
        "\n",
        "        # Eliminaciones opcionales\n",
        "        if eliminar_recetas_sin_spans and not r2[clave_spans]:\n",
        "            continue\n",
        "\n",
        "        if eliminar_si_ingredientes_en_resumen_vacio:\n",
        "            ing_res = r2.get(\"ingredientes_en_resumen\")\n",
        "            if not ing_res:  # None, [], \"\", etc. se consideran vac√≠os\n",
        "                continue\n",
        "\n",
        "        nuevas_recetas.append(r2)\n",
        "\n",
        "    return nuevas_recetas"
      ],
      "metadata": {
        "id": "ukPOEOByHZks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recetas_filtradas = filtrar_spans_por_frecuentes(recetas_cargadas, ingredientes_frecuentes=ingredientes_frecuentes)"
      ],
      "metadata": {
        "id": "dLwje5PZHdQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la simplificaci√≥n de los ingredientes repetidos/parecidos, realizamos un remapeo de las etiquetas:\n",
        "\n",
        "- 'aceite de oliva' -> 'aceite'\n",
        "- 'caldo de pollo' -> 'caldo'\n",
        "- 'caldo de pescado' -> 'caldo\n",
        "\n",
        "Se utilizan las funciones:\n",
        "\n",
        "- ***_norm***: normaliza una cadena quitando tildes, pasando a min√∫sculas, cambiando ‚Äú_‚Äù por espacio y colapsando/recortando espacios.\n",
        "\n",
        "- ***_dedup_preservando_orden***: elimina duplicados (y elementos vac√≠os) de una secuencia de strings preservando el primer orden de aparici√≥n.\n",
        "\n",
        "- ***remap_labels_en_recetas***: aplica un mapa de reemplazos de labels  sobre varios los campos span e ingredeinetes de cada receta, deduplica listas y cuenta los cambios y devuelve (nuevas_recetas, num_reemplazos).\n",
        "\n"
      ],
      "metadata": {
        "id": "vEtFfaOc3_ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para remapear\n",
        "import unicodedata\n",
        "from typing import  Tuple, Iterable, Union\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    # min√∫sculas, sin tildes, \"_\"‚Üí\" \", colapsa espacios\n",
        "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
        "    s = s.lower().replace(\"_\", \" \").strip()\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def _dedup_preservando_orden(xs: Iterable[str]) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    for x in xs:\n",
        "        if x and x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "def remap_labels_en_recetas(\n",
        "    recetas: List[Dict[str, Any]],\n",
        "    reemplazos: Union[Dict[str, str], Tuple[str, str], List[Tuple[str, str]]],\n",
        "    clave_spans: str = \"spans_resumen\",\n",
        "    clave_ing: str = \"ingredient\",\n",
        "    tocar_ingredientes_en_resumen: bool = True,\n",
        "    tocar_ingredientes: bool = True,\n",
        ") -> Tuple[List[Dict[str, Any]], int]:\n",
        "    \"\"\"\n",
        "    Aplica remapeos de labels (p.ej., {'aceite de oliva':'aceite', 'piment√≥n dulce':'piment√≥n'})\n",
        "    sobre TODAS las recetas en: spans_resumen, ingredientes_en_resumen e ingredientes.\n",
        "\n",
        "    Devuelve (nuevas_recetas, num_reemplazos).\n",
        "    \"\"\"\n",
        "\n",
        "    # Normaliza 'reemplazos' a dict\n",
        "    if isinstance(reemplazos, tuple) and len(reemplazos) == 2:\n",
        "        reemplazos = {reemplazos[0]: reemplazos[1]}\n",
        "    elif isinstance(reemplazos, list):\n",
        "        reemplazos = {a: b for (a, b) in reemplazos}\n",
        "\n",
        "    if not isinstance(reemplazos, dict):\n",
        "        raise ValueError(\"Argumento 'reemplazos' debe ser dict, (old,new) o lista de tuplas [(old,new), ...].\")\n",
        "\n",
        "    # Mapa normalizado (insensible a tildes/case/_)\n",
        "    norm_map: Dict[str, str] = {}\n",
        "    for src, dst in reemplazos.items():\n",
        "        if isinstance(src, str) and isinstance(dst, str):\n",
        "            norm_map[_norm(src)] = dst.strip()\n",
        "\n",
        "    cambios = 0\n",
        "    nuevas: List[Dict[str, Any]] = []\n",
        "\n",
        "    for r in recetas:\n",
        "        r2 = dict(r)\n",
        "\n",
        "        # 1) spans_resumen\n",
        "        spans = r2.get(clave_spans) or []\n",
        "        nuevos_spans = []\n",
        "        for s in spans:\n",
        "            if not isinstance(s, dict):\n",
        "                continue\n",
        "            s2 = dict(s)\n",
        "            lab = s2.get(clave_ing)\n",
        "            key = _norm(lab) if isinstance(lab, str) else None\n",
        "            if key in norm_map:\n",
        "                s2[clave_ing] = norm_map[key]\n",
        "                cambios += 1\n",
        "            nuevos_spans.append(s2)\n",
        "        r2[clave_spans] = sorted(nuevos_spans, key=lambda x: (x.get(\"start\", 0), x.get(\"end\", 0)))\n",
        "\n",
        "        # 2) ingredientes_en_resumen\n",
        "        if tocar_ingredientes_en_resumen and \"ingredientes_en_resumen\" in r2:\n",
        "            lst = r2.get(\"ingredientes_en_resumen\") or []\n",
        "            lst2 = []\n",
        "            for lab in lst:\n",
        "                key = _norm(lab) if isinstance(lab, str) else None\n",
        "                lst2.append(norm_map.get(key, lab))\n",
        "                if key in norm_map:\n",
        "                    cambios += 1\n",
        "            r2[\"ingredientes_en_resumen\"] = _dedup_preservando_orden(lst2)\n",
        "\n",
        "        # 3) ingredientes (lista can√≥nica)\n",
        "        if tocar_ingredientes and \"ingredientes\" in r2:\n",
        "            lst = r2.get(\"ingredientes\") or []\n",
        "            lst2 = []\n",
        "            for lab in lst:\n",
        "                key = _norm(lab) if isinstance(lab, str) else None\n",
        "                lst2.append(norm_map.get(key, lab))\n",
        "                if key in norm_map:\n",
        "                    cambios += 1\n",
        "            r2[\"ingredientes\"] = _dedup_preservando_orden(lst2)\n",
        "\n",
        "        nuevas.append(r2)\n",
        "\n",
        "    return nuevas, cambios"
      ],
      "metadata": {
        "id": "RYMhcsNk30ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = {\n",
        "    \"aceite de oliva\": \"aceite\",\n",
        "    \"caldo de pescado\": \"caldo\",\n",
        "    \"caldo de pollo\": \"caldo\",\n",
        "}\n",
        "recetas_remap, n = remap_labels_en_recetas(recetas_filtradas, mapa)\n",
        "print(\"Reemplazos:\", n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLUCd0nj43J5",
        "outputId": "355b1438-7698-403e-a419-2c2e157471ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reemplazos: 1546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A modo de ejemplo se muestra una entrada de la lista, correspondiente a la receta \"Ensalada de pasta\", una vez aplicado el remapeo de ingredientes parecidos (en este caso aceite de oliva) + eliminaci√≥n de ingredientes conflictivos (en este caso pimienta)"
      ],
      "metadata": {
        "id": "1LCabi35P6w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recetas_remap[241]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfNI2vaN48fy",
        "outputId": "0f89021d-448d-4318-f42c-fa94113de317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'titulo': 'Ensalada de pasta, un cl√°sica receta de verano',\n",
              " 'url': 'https://recetasdecocina.elmundo.es/2021/07/ensalada-pasta-receta-verano.html',\n",
              " 'ingredientes': ['pasta',\n",
              "  'ma√≠z',\n",
              "  'at√∫n',\n",
              "  'huevo',\n",
              "  'pimiento',\n",
              "  'can√≥nigo',\n",
              "  'sal',\n",
              "  'pimienta',\n",
              "  'aceite',\n",
              "  'vinagre',\n",
              "  'comino',\n",
              "  'or√©gano'],\n",
              " 'pasos': ['Comenzamos cociendo la pasta en abundante agua con un poquito de sal como indique el fabricante.',\n",
              "  'Una vez tengamos la pasta cocida. La pasamos por agua fr√≠a y as√≠ cortamos la cocci√≥n para que tengamos la pasta fr√≠a y no templada.',\n",
              "  'Ahora por otra parte cocemos los huevos en agua durante 10 minutos.',\n",
              "  'Seguidamente picamos el resto de ingredientes que haya que picar y sacamos de la lata el ma√≠z, el at√∫n etc‚Ä¶',\n",
              "  'Mezclamos muy bien todo y ali√±amos con aceite de oliva y vinagre. Ponemos las especias que m√°s nos gusten.'],\n",
              " 'resumen': 'Comenzamos cociendo la pasta en abundante agua con un poquito de sal como indique el fabricante. Una vez tengamos la pasta cocida. La pasamos por agua fr√≠a y as√≠ cortamos la cocci√≥n para que tengamos la pasta fr√≠a y no templada. Ahora por otra parte cocemos los huevos en agua durante 10 minutos. Seguidamente picamos el resto de ingredientes que haya que picar y sacamos de la lata el ma√≠z, el at√∫n etc. Mezclamos muy bien todo y ali√±amos con aceite de oliva y vinagre. Ponemos las especias que m√°s nos gusten.',\n",
              " 'ingredientes_en_resumen': ['pasta',\n",
              "  'sal',\n",
              "  'huevo',\n",
              "  'at√∫n',\n",
              "  'aceite',\n",
              "  'vinagre'],\n",
              " 'spans_resumen': [{'ingredient': 'pasta',\n",
              "   'start': 23,\n",
              "   'end': 28,\n",
              "   'surface': 'pasta'},\n",
              "  {'ingredient': 'sal', 'start': 65, 'end': 68, 'surface': 'sal'},\n",
              "  {'ingredient': 'pasta', 'start': 117, 'end': 122, 'surface': 'pasta'},\n",
              "  {'ingredient': 'pasta', 'start': 203, 'end': 208, 'surface': 'pasta'},\n",
              "  {'ingredient': 'huevo', 'start': 262, 'end': 268, 'surface': 'huevos'},\n",
              "  {'ingredient': 'at√∫n', 'start': 395, 'end': 399, 'surface': 'at√∫n'},\n",
              "  {'ingredient': 'aceite',\n",
              "   'start': 444,\n",
              "   'end': 459,\n",
              "   'surface': 'aceite de oliva'},\n",
              "  {'ingredient': 'vinagre', 'start': 462, 'end': 469, 'surface': 'vinagre'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que ya tenemos las recetas filtradas, reestructuramos los diccionarios para darles un formato adecuado para el entrenamiento de NER"
      ],
      "metadata": {
        "id": "oqk_6GB8Qb_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _resolve_overlaps_longest(spans):\n",
        "    if not spans: return []\n",
        "    spans = sorted(spans, key=lambda s: (s[\"start\"], -(s[\"end\"]-s[\"start\"])))\n",
        "    out = []\n",
        "    for s in spans:\n",
        "        if not out or s[\"start\"] >= out[-1][\"end\"]:\n",
        "            out.append(s)\n",
        "        # si solapa, se queda el previo (m√°s largo, ya que ordenamos por longitud desc)\n",
        "    return out\n",
        "\n",
        "def recetas_a_ner_multiclase(recetas, resolver_solapes=True):\n",
        "    \"\"\"\n",
        "    Usa el campo 'ingredient' de cada span como label (ya normalizado).\n",
        "    Formato de salida: [{id, text, entities:[{start,end,label}, ...]}, ...]\n",
        "    \"\"\"\n",
        "    ejemplos = []\n",
        "    label_set = set()\n",
        "\n",
        "    for i, r in enumerate(recetas, 1):\n",
        "        text = r.get(\"resumen\") or \"\"\n",
        "        spans = r.get(\"spans_resumen\") or []\n",
        "        use = _resolve_overlaps_longest(spans) if resolver_solapes else sorted(spans, key=lambda s:(s[\"start\"], s[\"end\"]))\n",
        "\n",
        "        entities = []\n",
        "        for s in use:\n",
        "            start, end = int(s[\"start\"]), int(s[\"end\"])\n",
        "            # validaci√≥n suave de offsets\n",
        "            surface = text[start:end]\n",
        "            if \"surface\" in s and s[\"surface\"] != surface:\n",
        "                # si no cuadra, puedes loguear o continuar\n",
        "                pass\n",
        "            lab = s.get(\"ingredient\", \"\")\n",
        "            if not lab:\n",
        "                continue\n",
        "            entities.append({\"start\": start, \"end\": end, \"label\": lab})\n",
        "            label_set.add(lab)\n",
        "\n",
        "        ejemplos.append({\n",
        "            \"id\": r.get(\"id\", f\"receta_{i:05d}\"),\n",
        "            \"text\": text,\n",
        "            \"entities\": entities\n",
        "        })\n",
        "\n",
        "    return ejemplos, sorted(label_set)"
      ],
      "metadata": {
        "id": "F9urDG0kSyiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, labels = recetas_a_ner_multiclase(recetas_remap)"
      ],
      "metadata": {
        "id": "O0wb6KdvVewB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#guardamos el dataset que servir√° de entrenamiento (+ validacion + test) del modelo NER\n",
        "Path(\"train_data_2.json\").write_text(\n",
        "    json.dumps(train_data, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTJY0QnNkJR",
        "outputId": "cc720d94-e0df-4f20-ad2f-ad12fb43208e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1283963"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leerlo de vuelta para chequear que todo esta bien\n",
        "contenido = Path(\"train_data_2.json\").read_text(encoding=\"utf-8\")\n",
        "data_train = json.loads(contenido)"
      ],
      "metadata": {
        "id": "BL10sLSuVn2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[922]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2flWRQEOe-I",
        "outputId": "18c264f5-a386-443f-ca4d-3c655c3b0902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'receta_00923',\n",
              " 'text': 'En un bol preparamos las hamburguesas, mezclamos la carne, con el huevo, el or√©gano, el ajo picado, el pan rallado, la mostaza en grano, sal y pimienta. Ahora como veis en la foto pon√©is la carne picada macerada en una tabla y aplast√°is con la mano haciendo la forma redonda. Con la ayuda de un molde hacemos un agujero y retiramos la carne. Mirad que bien quedan üôÇ. Las ponemos en la plancha por una parte. Una vez que le damos la vuelta las ponemos un poco m√°s lentas y ponemos el huevo frito dentro. Pod√©is taparlas as√≠ el huevo frito se har√° con el calor m√°s r√°pidamente.',\n",
              " 'entities': [{'start': 66, 'end': 71, 'label': 'huevo'},\n",
              "  {'start': 88, 'end': 91, 'label': 'ajo'},\n",
              "  {'start': 137, 'end': 140, 'label': 'sal'},\n",
              "  {'start': 483, 'end': 488, 'label': 'huevo'},\n",
              "  {'start': 526, 'end': 531, 'label': 'huevo'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}